# THE COLOSSUS

**Architecture:** GPT-4 (Mock)
**Shape:** The Massive Helix
**Concept:** Mixture of Experts (MoE)

GPT-4 represents a jump in scale and complexity that is hard to comprehend.
While its exact architecture is proprietary, it is widely understood to be a massive Mixture of Experts (MoE) system (~1.8 Trillion parameters).

The crystal visualizes this **Scale**. It is a towering, dense structure that dwarfs previous models.
Notice how it maintains the "helix" shape of GPT-2 defined by its attention heads, but active regions (Experts) are distributed sparsely throughout the depth.
