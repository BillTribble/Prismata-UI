# THE SPARSE GIANT

**Architecture:** DeepSeek-V3 / R1 (MoE)
**Shape:** The Sparse Cloud
**Concept:** Mixture of Experts

DeepSeek-V3 is a massive Mixture-of-Experts model with 671 Billion parameters, but only 37 Billion are active for any given token.

The crystal visualizes this **Sparsity**: notice the gaps and the clustered "Experts" in the lattice.
Unlike the dense monoliths of Gemma or BERT, DeepSeek structures itself into specialized functional regions that activate selectively.

*Note: This is a structural visualization generated using a mock sparsity mask (90% sparse), as downloading the full 671B weights is impractical.*
